{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, cross_val_predict, train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, balanced_accuracy_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "#import loralib as lora\n",
    "import random\n",
    "#import umap\n",
    "import re\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ca7a2",
   "metadata": {},
   "source": [
    "### Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteomics_data = pd.read_csv('./data/processed/processed_proteomics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteomics_data.index = proteomics_data['UniProt']\n",
    "proteomics_data = proteomics_data.drop(['Symbol','UniProt'],axis=1).T\n",
    "proteomics_data.index.name = 'sample_ID'\n",
    "proteomics_data = proteomics_data.reset_index()\n",
    "proteomics_data['sample_ID'] = proteomics_data['sample_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class proteomics_net(nn.Module):\n",
    "    def __init__(self, input_size, proteomics_hidden_layers, output_size, dropout=0.1):\n",
    "        super(proteomics_net, self).__init__()\n",
    "        \n",
    "        self.proteomics_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proteomics_layers(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, dropout=0.1):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths, interpretability=False):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        out_final = self.prediction_module(out)\n",
    "        if interpretability == False:\n",
    "            return out_final\n",
    "        else:\n",
    "            return out_final, out\n",
    "\n",
    "# Prepare the dataset\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    data, labels, lengths = zip(*batch)\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return data, labels, lengths\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        dataset[i] = (sequence - sequence.mean(dim=0, keepdim=True)) / (sequence.std(dim=0, keepdim=True) + 1e-8)\n",
    "    return dataset\n",
    "\n",
    "def impute_missing_values(dataset):\n",
    "    # Stack all tensors in the dataset along a new dimension, creating a tensor of shape (num_samples, max_seq_length, num_features)\n",
    "    stacked_data = torch.stack(dataset)\n",
    "\n",
    "    # Calculate the mean of each feature across all samples and sequences, ignoring NaN values\n",
    "    feature_means = torch.nanmean(stacked_data, dim=(0, 1))\n",
    "\n",
    "    # Iterate through the dataset (list of tensors)\n",
    "    for i, sequence in enumerate(dataset):\n",
    "        # Create a boolean mask indicating the positions of NaN values in the sequence\n",
    "        mask = torch.isnan(sequence)\n",
    "\n",
    "        # Replace NaN values in the sequence with the corresponding feature means\n",
    "        # 'expand_as' is used to match the dimensions of the mask and the sequence\n",
    "        dataset[i][mask] = feature_means.expand_as(sequence)[mask]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_dataloaders(patient_data, patient_outcomes, lengths, batch_size=64, normalize=False):\n",
    "    \n",
    "    X_train = impute_missing_values(patient_data)\n",
    "    y_train = patient_outcomes\n",
    "    \n",
    "    if normalize:\n",
    "        X_train = normalize_dataset(X_train)\n",
    "        \n",
    "    train_dataset = PatientDataset(X_train, y_train, lengths)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=worker_init_fn)\n",
    "    return train_loader\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ee95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model with GRU units\n",
    "class joint_model(nn.Module):\n",
    "    def __init__(self, input_size_codes, hidden_size, prediction_module_hidden_sizes, num_layers, output_size, input_size_proteomics, proteomics_hidden_layers, combined_hidden_layers, dropout=0.1):\n",
    "        super(joint_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size_codes, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "    \n",
    "        self.prediction_module = nn.Sequential(\n",
    "            nn.Linear(prediction_module_hidden_sizes[0], output_size)\n",
    "        )\n",
    "\n",
    "        self.skip_connect_prot = nn.Linear(input_size_proteomics, output_size)\n",
    "        \n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(input_size_proteomics + hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        self.final_combine = nn.Linear(3, 1, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, x_proteomics, lengths, interpretability=False, better_latent=None, better_ratio=0.5):\n",
    "        device = x.device\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.gru(x_packed, h0)\n",
    "        out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        out_ehr = out[torch.arange(x.size(0)), lengths-1, :]\n",
    "        \n",
    "        if x_proteomics == None:\n",
    "            return out_ehr\n",
    "\n",
    "        if better_latent is not None:\n",
    "            out_ehr = better_ratio * better_latent + (1-better_ratio) * out_ehr\n",
    "                \n",
    "        out_combined = torch.cat((out_ehr, x_proteomics), 1)\n",
    "        \n",
    "        out_combined = self.combined_layers(out_combined)\n",
    "        \n",
    "        pred_proteomics = self.skip_connect_prot(x_proteomics)\n",
    "        pred_ehr = self.prediction_module(out_ehr)\n",
    "        \n",
    "        final_pred = torch.sigmoid(self.final_combine(torch.cat((pred_proteomics, pred_ehr, out_combined), 1)))\n",
    "#         final_pred = self.final_combine(torch.cat((pred_proteomics, pred_ehr), 1))\n",
    "        \n",
    "        if interpretability == False:\n",
    "            return final_pred\n",
    "        else:\n",
    "            return final_pred, (out_ehr, pred_proteomics, pred_ehr, out_combined, final_pred, self.final_combine.weight)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ccc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, x, y, standardizer):\n",
    "        self.x, self.y, self.standardizer = x, y, standardizer\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return (self.x[index], self.y[index])\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42037cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numpy(mat, column_indices):\n",
    "    for col_idx in column_indices:\n",
    "        col_mean = np.nanmean(mat[:, col_idx])  # Compute mean of the column, ignoring nan values\n",
    "        nan_idx = np.isnan(mat[:, col_idx])  # Get boolean mask of nan values in the column\n",
    "        mat[nan_idx, col_idx] = col_mean  # Replace nan values with the column mean\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(EHR_codes, proteomics, patient_indices, outcomes, lengths, experiment_name, lr, lr_decay,\n",
    "                   bs, train_indices=None, val_indices=None, test_indices=None, feature_types='EHR', model_path='', fine_tune=False, seed=42, num_layers=2,hidden_dim=400,\n",
    "                   dropout=0.4, return_preds=False, return_interpretability=False, return_grads=False,\n",
    "                   protein_weights=None, blend=None, hyperparam_tuning=False):\n",
    "    \"\"\"\n",
    "    EHR_codes: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)v\n",
    "    EHR_vitals: pre-processed data for codes of shape (num_patients, max_length, embedding_dim)\n",
    "    proteomics: dataframe with proteomics data\n",
    "    patient_indices: dataframe with sample IDs and row numbers in pre-processed matrices\n",
    "    outcomes: array with DOS\n",
    "    lengths: array with lengths (i.e. number of visits) to help with padding\n",
    "    experiment_name: string for file name for models\n",
    "    lr: float for learning rate\n",
    "    lr_decay: float for learning rate decay\n",
    "    bs: int for batch size\n",
    "    feature_types: string either 'EHR', 'metab', 'both'\n",
    "    model_path: string for file path to model if loading a pre-trained model\n",
    "    fine_tune: boolean for whether or not EHR weight should be learned, can only be true if model != ''\n",
    "    seed: int, random_seed for train/test/val split and seeding model \n",
    "    ...\n",
    "    blend: if True, blend noise to PT model; if file path, blend real weights to NPT model (not implemented yet)\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    prediction_module_hidden_sizes = [hidden_dim,hidden_dim//2, hidden_dim//4, hidden_dim//8]\n",
    "    \n",
    "    assert feature_types in ['EHR','metab','both']   \n",
    "    if feature_types == 'metab': assert model_path == ''\n",
    "    if (model_path != '') & (feature_types == 'both'): assert fine_tune==True\n",
    "    if hyperparam_tuning == False: assert train_indices == None\n",
    "\n",
    "    \n",
    "    if hyperparam_tuning == False:\n",
    "        maternal_IDs = patient_indices['sample_ID']\n",
    "\n",
    "        train_ratio = 0.70\n",
    "        test_ratio = 0.15\n",
    "        val_ratio = 0.15\n",
    "\n",
    "        # First, split the unique_ids into train and temp (test + validation) sets\n",
    "        train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio),random_state=seed, stratify=outcomes)\n",
    "        \n",
    "        patient_indices_temp = patient_indices.copy(deep=True)\n",
    "\n",
    "        patient_indices_temp.index = patient_indices_temp['sample_ID']\n",
    "        temp_indices = patient_indices_temp.loc[temp_ids,'array_index'].values\n",
    "\n",
    "        # Next, split the temp_ids into test and validation sets\n",
    "        test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=seed, stratify=outcomes[temp_indices])\n",
    "        proteomics = proteomics.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_ids)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_ids)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_ids)]['array_index'].values\n",
    "        counter = seed+100\n",
    "        while (len(train_indices) % bs == 1) | (len(test_indices) % bs == 1) | (len(val_indices) % bs == 1):\n",
    "            counter += 1\n",
    "            if counter > 150:\n",
    "                print('stuck!')\n",
    "                break\n",
    "            train_ids, temp_ids = train_test_split(maternal_IDs, test_size=(test_ratio + val_ratio), random_state=counter)\n",
    "            test_ids, val_ids = train_test_split(temp_ids, test_size=(val_ratio / (test_ratio + val_ratio)), random_state=counter)\n",
    "            train_indices = patient_indices[patient_indices['sample_ID'].isin(train_ids)]['array_index'].values\n",
    "            np.random.shuffle(train_indices)\n",
    "            test_indices = patient_indices[patient_indices['sample_ID'].isin(test_ids)]['array_index'].values\n",
    "            val_indices = patient_indices[patient_indices['sample_ID'].isin(val_ids)]['array_index'].values\n",
    "\n",
    "    train_EHR_codes = EHR_codes[train_indices,:,:]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = proteomics[train_indices,:]\n",
    "        scaler = StandardScaler()\n",
    "        train_proteomics = scaler.fit_transform(train_proteomics)\n",
    "    train_outcomes = outcomes[train_indices]\n",
    "    train_lengths = lengths[train_indices]\n",
    "    \n",
    "    test_EHR_codes = EHR_codes[test_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = proteomics[test_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        test_proteomics = scaler.fit_transform(test_proteomics)\n",
    "    test_outcomes = outcomes[test_indices]\n",
    "    test_lengths = lengths[test_indices]\n",
    "\n",
    "    val_EHR_codes = EHR_codes[val_indices, :, :]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = proteomics[val_indices, :]\n",
    "        scaler = StandardScaler()\n",
    "        val_proteomics = scaler.fit_transform(val_proteomics)\n",
    "    val_outcomes = outcomes[val_indices]\n",
    "    val_lengths = lengths[val_indices]\n",
    "        \n",
    "    all_EHR_codes = EHR_codes\n",
    "    scaler = StandardScaler()\n",
    "    all_outcomes = outcomes\n",
    "    all_lengths = lengths\n",
    "\n",
    "    train_EHR_codes = [torch.tensor(data).float() for data in train_EHR_codes]  \n",
    "    train_EHR_codes = [torch.nan_to_num(x) for x in train_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        train_proteomics = torch.tensor(train_proteomics).float()\n",
    "        train_proteomics = torch.nan_to_num(train_proteomics)\n",
    "    train_outcomes = torch.tensor(train_outcomes).float()\n",
    "    \n",
    "        # Test sets\n",
    "    test_EHR_codes = [torch.tensor(data).float() for data in test_EHR_codes]\n",
    "    test_EHR_codes = [torch.nan_to_num(x) for x in test_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        test_proteomics = torch.tensor(test_proteomics).float()\n",
    "        test_proteomics = torch.nan_to_num(test_proteomics)\n",
    "    test_outcomes = torch.tensor(test_outcomes).float()\n",
    "\n",
    "    # Validation sets\n",
    "    val_EHR_codes = [torch.tensor(data).float() for data in val_EHR_codes]\n",
    "    val_EHR_codes = [torch.nan_to_num(x) for x in val_EHR_codes]\n",
    "    if feature_types != 'EHR':\n",
    "        val_proteomics = torch.tensor(val_proteomics).float()\n",
    "        val_proteomics = torch.nan_to_num(val_proteomics)\n",
    "    val_outcomes = torch.tensor(val_outcomes).float()\n",
    "    \n",
    "    all_EHR_codes = [torch.tensor(data).float() for data in all_EHR_codes]\n",
    "    all_EHR_codes = [torch.nan_to_num(x) for x in all_EHR_codes]\n",
    "    all_outcomes = torch.tensor(all_outcomes).float()\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_train = DataBuilder(train_proteomics, train_outcomes, scaler)\n",
    "        train_loader_proteomics = DataLoader(dataset=data_set_train,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    train_loader_codes = create_dataloaders(train_EHR_codes, train_outcomes, train_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_test = DataBuilder(test_proteomics, test_outcomes, scaler)\n",
    "        test_loader_proteomics = DataLoader(dataset=data_set_test,batch_size=bs, worker_init_fn=worker_init_fn)\n",
    "    test_loader_codes = create_dataloaders(test_EHR_codes, test_outcomes, test_lengths, bs)\n",
    "\n",
    "    if feature_types != 'EHR':\n",
    "        data_set_val = DataBuilder(val_proteomics, val_outcomes, scaler)\n",
    "        val_loader_proteomics = DataLoader(dataset=data_set_val,batch_size=100*bs, worker_init_fn=worker_init_fn)\n",
    "    val_loader_codes = create_dataloaders(val_EHR_codes, val_outcomes, val_lengths, bs)\n",
    "            \n",
    "    all_loader_codes = create_dataloaders(all_EHR_codes, all_outcomes, all_lengths, 1000)   \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if model_path == '':\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout).to(device)\n",
    "        elif feature_types == 'metab':\n",
    "            model = proteomics_net(proteomics.shape[1], [1024, 512, 256, 128], 1, dropout).to(device)\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "            \n",
    "    else:\n",
    "        if feature_types == 'EHR':\n",
    "            model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            model.to(device)\n",
    "            if fine_tune == False:\n",
    "                model.eval()\n",
    "                criterion = nn.BCELoss()\n",
    "                val_predictions = []\n",
    "                val_true_labels = []\n",
    "                running_loss_val, num_samples_val = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for (inputs_codes, labels_codes, lengths_codes) in (val_loader_codes):\n",
    "                            inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                            outputs = model(inputs_codes, lengths_codes)\n",
    "\n",
    "                            loss = criterion(outputs.squeeze(), labels)\n",
    "                            running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                            num_samples_val += lengths_codes.shape[0]\n",
    "                            val_predictions.extend(outputs.squeeze().tolist())\n",
    "                            val_true_labels.extend(labels.tolist())\n",
    "                    \n",
    "                val_loss = running_loss_val / (num_samples_val)\n",
    "                pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "\n",
    "                print(f'Total Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}')\n",
    "                if return_preds == True:\n",
    "                    return pearson_corr, val_loss, None, val_true_labels, val_predictions, val_indices\n",
    "                else:\n",
    "                    return pearson_corr, val_loss, None\n",
    "            elif fine_tune == True:\n",
    "                model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "                model_state_dict = torch.load(model_path)\n",
    "                model.load_state_dict(model_state_dict, strict=False)\n",
    "                model.to(device)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if ('gru' in name):\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        elif feature_types == 'both':\n",
    "            model = joint_model(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, proteomics.shape[1], [1024, 512, 256, 128], [64, 32, 16, 8], dropout).to(device)\n",
    "            model_state_dict = torch.load(model_path)\n",
    "            gru_weights = {}\n",
    "            for k,v in zip(model_state_dict.keys(), model_state_dict.values()):\n",
    "                if ('gru' in k) | ('pred' in k):\n",
    "                    gru_weights[k] = v\n",
    "            model.load_state_dict(gru_weights, strict=False)\n",
    "            model.to(device)\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                if ('gru' in name):\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    if protein_weights != None:\n",
    "        model_state_dict = torch.load(protein_weights)\n",
    "        protein_weight_dict = {}\n",
    "        for k,v in zip(model_state_dict.keys(), model_state_dict.values()):\n",
    "            if ('skip_connect' in k) | ('combined_layers' in k):\n",
    "                protein_weight_dict[k] = v\n",
    "        model.load_state_dict(protein_weight_dict, strict=False)\n",
    "                \n",
    "   \n",
    "    if blend != None:\n",
    "        PT_EHR_model = GRUNet(EHR_codes.shape[2], hidden_dim, prediction_module_hidden_sizes, num_layers, 1, dropout)\n",
    "        model_state_dict = torch.load(blend)\n",
    "        PT_EHR_model.load_state_dict(model_state_dict)\n",
    "        PT_EHR_model.to(device)\n",
    "        PT_EHR_model.eval()\n",
    "\n",
    "        \n",
    "    epoch_arr = []\n",
    "    num_samples_in_batch = []\n",
    "    gradient_arr = []\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr_decay)\n",
    "    num_epochs = 200\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        if hyperparam_tuning == False:\n",
    "            torch.cuda.synchronize()  # Wait for all CUDA kernels to finish\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}_epoch{}.pth'.format(experiment_name, epoch))\n",
    "        running_loss_train, num_train_samples = 0, 0\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes)in train_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if blend != None:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = PT_EHR_model(inputs_codes, lengths_codes, True)\n",
    "                        optimal_latent = outputs[1]\n",
    "                    outputs = model(inputs_codes, lengths_codes, better_latent=optimal_latent, better_ratio=0.5)\n",
    "                \n",
    "                else:\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(train_loader_codes, train_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if return_grads:\n",
    "                    epoch_arr.append(epoch)\n",
    "                    num_samples_in_batch.append(outputs.shape[0])\n",
    "                    gradient_arr.append(model.skip_connect_prot.weight.grad.cpu().numpy()[0])\n",
    "                running_loss_train += (loss.item()*lengths_codes.shape[0])\n",
    "                num_train_samples += lengths_codes.shape[0]\n",
    "\n",
    "        train_loss = running_loss_train / num_train_samples\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss_test, num_samples_test = 0, 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        if feature_types == 'EHR':\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes)in test_loader_codes:\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(test_loader_codes, test_loader_proteomics)):\n",
    "                    if feature_types != 'metab':\n",
    "                        inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                    if feature_types != 'EHR':\n",
    "                        inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                    if feature_types == 'metab':\n",
    "                        outputs = model(inputs_proteomics)\n",
    "                    elif feature_types == 'EHR':\n",
    "                        outputs = model(inputs_codes, lengths_codes)\n",
    "                    elif feature_types == 'both':\n",
    "                        outputs = model(inputs_codes, inputs_proteomics, lengths_codes)\n",
    "                    \n",
    "                    outputs = outputs.squeeze(1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    running_loss_test += (loss.item()*lengths_codes.shape[0])\n",
    "                    num_samples_test += lengths_codes.shape[0]\n",
    "                    predictions.extend(outputs.tolist())\n",
    "                    true_labels.extend(labels.tolist())\n",
    "        test_loss = running_loss_test / (num_samples_test)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        patience = 5\n",
    "        pearson_corr = roc_auc_score(true_labels, predictions)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Pearson R: {pearson_corr:.4f}')\n",
    "        \n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "            torch.cuda.synchronize()  # Wait for all CUDA kernels to finish\n",
    "            torch.save(model.state_dict(), './models/predictive_models/{}.pth'.format(experiment_name))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load('./models/predictive_models/{}.pth'.format(experiment_name)))\n",
    "    running_loss_val, num_samples_val = 0, 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        if feature_types == 'EHR':\n",
    "            for (inputs_codes, labels_codes, lengths_codes) in val_loader_codes:\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "                    \n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "        else:\n",
    "            for (inputs_codes, labels_codes, lengths_codes), (inputs_proteomics, labels_proteomics) in (zip(val_loader_codes, val_loader_proteomics)):\n",
    "                if feature_types != 'metab':\n",
    "                    inputs_codes, labels = inputs_codes.to(device), labels_codes.to(device)\n",
    "                if feature_types != 'EHR':\n",
    "                    inputs_proteomics, labels = inputs_proteomics.to(device), labels_proteomics.to(device)\n",
    "\n",
    "                if feature_types == 'metab':\n",
    "                    outputs = model(inputs_proteomics)\n",
    "                elif feature_types == 'EHR':\n",
    "                    outputs = model(inputs_codes, lengths_codes)\n",
    "                elif feature_types == 'both':\n",
    "                    outputs, interpretability_outputs = model(inputs_codes, inputs_proteomics, lengths_codes, interpretability=True)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                running_loss_val += (loss.item()*lengths_codes.shape[0])\n",
    "                num_samples_val += lengths_codes.shape[0]\n",
    "                val_predictions.extend(outputs.squeeze().tolist())\n",
    "                val_true_labels.extend(labels.tolist())\n",
    "    val_loss = running_loss_val / (num_samples_val)\n",
    "    val_losses.append(val_loss)\n",
    "    pearson_corr = roc_auc_score(val_true_labels, val_predictions)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Pearson R: {pearson_corr:.4f}')\n",
    "    \n",
    "    if hyperparam_tuning == True:\n",
    "        os.remove('./models/predictive_models/{}.pth'.format(experiment_name))\n",
    "    if return_preds == True:\n",
    "        if return_interpretability == False:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices\n",
    "        else:\n",
    "            if return_grads:\n",
    "                df = pd.DataFrame([epoch_arr, num_samples_in_batch, gradient_arr]).T\n",
    "                df.columns = ['epoch', 'num_samples_in_batch','gradient']\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, interpretability_outputs, None, None, df, train_losses, test_losses, val_losses, train_indices, test_indices\n",
    "            else:\n",
    "                return pearson_corr, val_loss, None, val_outcomes, np.array(val_predictions), val_indices, interpretability_outputs, None, None\n",
    "    else:\n",
    "        return pearson_corr, val_loss, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_data_codes_omics = np.load('./data/processed/RNN_data_omics_cohort_disease_modeling.npy')\n",
    "RNN_data_outcomes_omics = np.load('./data/processed/RNN_data_outcomes_omics_cohort_disease_status.npy')\n",
    "RNN_data_lengths_omics = np.load('./data/processed/RNN_data_lengths_omics_cohort_disease_status.npy')\n",
    "patient_indices_omics = pd.read_csv('./data/processed/sampleID_indices_omics_cohort_disease_modeling.csv')\n",
    "patient_indices_omics.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_omics.shape, RNN_data_outcomes_omics.shape, RNN_data_lengths_omics.shape, patient_indices_omics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(RNN_data_outcomes_omics == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_data_codes = np.load('./data/processed/RNN_data_omics_cohort_disease_modeling_PT_word2vec_model.npy')\n",
    "RNN_data_outcomes = np.load('./data/processed/RNN_data_outcomes_omics_cohort_disease_status_PT_word2vec_model.npy')\n",
    "RNN_data_lengths = np.load('./data/processed/RNN_data_lengths_omics_cohort_disease_status_PT_word2vec_model.npy')\n",
    "patient_indices = pd.read_csv('./data/processed/sampleID_indices_omics_cohort_disease_modeling_PT_word2vec_model.csv')\n",
    "patient_indices.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes.shape, RNN_data_outcomes.shape, RNN_data_lengths.shape, patient_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0cccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78074",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_data_codes_PTMODEL = np.load('./data/processed/RNN_data_full_EHR_cohort_disease_classification.npy')\n",
    "RNN_data_outcomes_PTMODEL = np.load('./data/processed/RNN_data_outcomes_full_EHR_cohort_disease_classification.npy')\n",
    "RNN_data_lengths_PTMODEL = np.load('./data/processed/RNN_data_lengths_full_EHR_cohort_disease_classification.npy')\n",
    "patient_indices_PTMODEL = pd.read_csv('./data/processed/sampleID_indices_full_cohort_disease_classification.csv')\n",
    "patient_indices_PTMODEL.columns = ['sample_ID','array_index']\n",
    "RNN_data_codes_PTMODEL.shape, RNN_data_outcomes_PTMODEL.shape, RNN_data_lengths_PTMODEL.shape, patient_indices_PTMODEL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a96fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = ['DR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "grid_search = {\n",
    "    'batch_size': [512],\n",
    "    'lr': [1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]\n",
    "\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i] = {'PT': {}}\n",
    "\n",
    "    maternal_IDs = patient_indices_PTMODEL['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "\n",
    "        train_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_PTMODEL[patient_indices_PTMODEL['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            \n",
    "            model_name = 'PT_MODEL_{}_{}_{}_{}_{}'.format(layers,lr,lr_decay,dropout, hidden_dim)\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, proteomics_data,\n",
    "            patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, model_name, \n",
    "            lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices,feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "            hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('PT')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "model_name = 'PT_MODEL_{}_{}_{}_{}_{}'.format(num_layers,lr,lr_decay,dropout, hidden_dim)\n",
    "overall_best_params['DR']['PT'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs), 'model_name': model_name}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = pickle.load(open('./models/hyperparameters/best_hyperparams.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c22cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DR']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DR']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DR']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DR']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_PTMODEL, proteomics_data,\n",
    "    patient_indices_PTMODEL, RNN_data_outcomes_PTMODEL, RNN_data_lengths_PTMODEL, best_model_name, \n",
    "    overall_best_params['DR']['PT']['lr'], overall_best_params['DR']['PT']['lr_decay'],\n",
    "       overall_best_params['DR']['PT']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=42, \n",
    "    hidden_dim=best_hidden_dim, num_layers=best_num_layers, dropout=best_dropout, hyperparam_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54396d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {\n",
    "    'batch_size': [16],\n",
    "    'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'layers': [2, 4],\n",
    "    'hidden_dim': [400, 800]\n",
    "}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d38e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp1'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics_data.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = param_set['layers']\n",
    "            hidden_dim = param_set['hidden_dim']\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                                                       patient_indices_omics, RNN_data_outcomes_omics,\n",
    "                                                       RNN_data_lengths_omics, 'EHR_omics_only',\n",
    "                                                       lr, lr_decay, bs, \n",
    "                                                       train_indices=train_indices, test_indices=test_indices,\n",
    "                                                       val_indices=val_indices, feature_types='EHR', model_path='',\n",
    "                                                       fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                                                       num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DR']['exp1'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451de22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_omics = overall_best_params['DR']['exp1']['num_layers']\n",
    "best_dropout_omics = overall_best_params['DR']['exp1']['dropout']\n",
    "best_hidden_dim_omics = overall_best_params['DR']['exp1']['hidden_dim']\n",
    "best_num_layers_omics, best_dropout_omics, best_hidden_dim_omics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp2'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics_data.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout_omics\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_omics\n",
    "            hidden_dim = best_hidden_dim_omics\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'proteomics_omics_only', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices, val_indices=val_indices,\n",
    "                feature_types='metab', model_path='', fine_tune=False, seed=42, hidden_dim=hidden_dim,\n",
    "                num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 2')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df45814",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DR']['exp2'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae009db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835876b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbf387",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp3'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices_omics['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics_data.merge(patient_indices_omics[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices_omics[patient_indices_omics['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = param_set['dropout']\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers_omics\n",
    "            hidden_dim = best_hidden_dim_omics\n",
    "            print(param_set)\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes_omics, input_proteomics,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'both_omics_only', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='both', model_path='', fine_tune=False, seed=42,\n",
    "                hidden_dim=hidden_dim, num_layers=layers, dropout=dropout, hyperparam_tuning=True)\n",
    "\n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 3')\n",
    "        print('outcome {}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09387b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DR']['exp3'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907db95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3658bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp4'] = {}\n",
    "    results_dict = {}\n",
    "\n",
    "    # Iterate through the hyperparameter combinations\n",
    "    bs = 1000\n",
    "    lr = 0.1\n",
    "    dropout = best_dropout\n",
    "    lr_decay = 0.1\n",
    "    layers = best_num_layers\n",
    "    hidden_dim = best_hidden_dim\n",
    "    val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, proteomics_data,\n",
    "            patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT', \n",
    "            lr, lr_decay, bs, feature_types='EHR', model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                               fine_tune=False, seed=42,\n",
    "                                               hidden_dim=hidden_dim,\n",
    "                                              num_layers=layers, dropout=dropout, hyperparam_tuning=False)\n",
    "\n",
    "    results_dict[val_loss] = {'num_layers': layers,'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                            'hidden_dim': hidden_dim, 'batch_size': bs}\n",
    "\n",
    "    print('experiment 4')\n",
    "    print('outcome {}'.format(i))\n",
    "    overall_best_params[i]['exp4'] = results_dict[min(results_dict.keys())]\n",
    "    print(results_dict[min(results_dict.keys())])\n",
    "\n",
    "# Save the best hyperparameters to a pickle file\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp5'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "\n",
    "        input_proteomics = proteomics_data.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            \n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_OOL_PT', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices, feature_types='EHR',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                       hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 5')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab17472",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "overall_best_params['DR']['exp5'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3f424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738244",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'batch_size': [16],\n",
    "              'lr': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "              'lr_decay': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "all_params = [dict(zip(grid_search.keys(), values)) for values in product(*grid_search.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0369b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_layers_arr = []\n",
    "dropout_arr = []\n",
    "lr_arr = []\n",
    "lr_decay_arr = []\n",
    "hidden_dim_arr = []\n",
    "batch_size_arr = []\n",
    "split_num_arr = []\n",
    "loss_arr = []\n",
    "for i in tqdm(outcome_list):\n",
    "    overall_best_params[i]['exp6'] = {}\n",
    "\n",
    "    maternal_IDs = patient_indices['sample_ID'].unique()\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    split_num = 0\n",
    "    for train_index, test_index in tqdm(kf.split(maternal_IDs)):\n",
    "        split_num += 1\n",
    "        results_dict = {}\n",
    "        train_IDs = maternal_IDs[train_index]\n",
    "        test_IDs = maternal_IDs[test_index]\n",
    "        \n",
    "        sample_size = int(0.2 * len(train_index))\n",
    "        random_indices = np.random.choice(train_IDs.shape[0], sample_size, replace=False)\n",
    "        val_IDs = train_IDs[random_indices]\n",
    "        train_IDs = np.delete(train_IDs, random_indices)\n",
    "        \n",
    "        input_proteomics = proteomics_data.merge(patient_indices[['sample_ID','array_index']], how='left', left_on='sample_ID', right_on='sample_ID').drop(['sample_ID'],axis=1).sort_values('array_index').drop('array_index',axis=1).values\n",
    "\n",
    "\n",
    "        train_indices = patient_indices[patient_indices['sample_ID'].isin(train_IDs)]['array_index'].values\n",
    "        np.random.shuffle(train_indices)\n",
    "        test_indices = patient_indices[patient_indices['sample_ID'].isin(test_IDs)]['array_index'].values\n",
    "        val_indices = patient_indices[patient_indices['sample_ID'].isin(val_IDs)]['array_index'].values\n",
    "\n",
    "        # Iterate through the hyperparameter combinations\n",
    "        for param_set in tqdm(all_params):\n",
    "            bs = param_set['batch_size']\n",
    "            lr = param_set['lr']\n",
    "            dropout = best_dropout\n",
    "            lr_decay = param_set['lr_decay']\n",
    "            layers = best_num_layers\n",
    "            hidden_dim = best_hidden_dim\n",
    "            print(param_set)\n",
    "            val_r, val_loss, val_rmse = run_experiment(RNN_data_codes, input_proteomics,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT', \n",
    "                lr, lr_decay, bs, train_indices=train_indices, test_indices=test_indices,\n",
    "                val_indices=val_indices,feature_types='both',\n",
    "                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                fine_tune=True, seed=42, hidden_dim=hidden_dim, num_layers=layers, dropout=dropout,\n",
    "                                                        hyperparam_tuning=True)\n",
    "            \n",
    "            num_layers_arr.append(layers)\n",
    "            dropout_arr.append(dropout)\n",
    "            lr_arr.append(lr)\n",
    "            lr_decay_arr.append(lr_decay)\n",
    "            hidden_dim_arr.append(hidden_dim)\n",
    "            batch_size_arr.append(bs)\n",
    "            split_num_arr.append(split_num)\n",
    "            loss_arr.append(val_loss)\n",
    "\n",
    "        print('experiment 6')\n",
    "        print('outcome {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_df = pd.DataFrame([num_layers_arr, dropout_arr, lr_arr, lr_decay_arr, hidden_dim_arr, batch_size_arr, split_num_arr,loss_arr]).T\n",
    "hyperparam_df.columns = ['num_layers','dropout','lr','lr_decay','hidden_dim','bs','split_num','val_loss']\n",
    "hyperparam_df = hyperparam_df.groupby(['num_layers','dropout','lr','lr_decay','hidden_dim','bs']).mean()\n",
    "num_layers, dropout, lr, lr_decay, hidden_dim, bs = hyperparam_df['val_loss'].idxmin()\n",
    "\n",
    "print(np.min(hyperparam_df['val_loss']))\n",
    "print(num_layers, dropout, lr, lr_decay, hidden_dim, bs)\n",
    "\n",
    "overall_best_params['DR']['exp6'] = {'num_layers': int(num_layers),'lr': lr,'lr_decay': lr_decay, 'dropout': dropout,\n",
    "                                'hidden_dim': int(hidden_dim), 'batch_size': int(bs)}\n",
    "with open(\"./models/hyperparameters/best_hyperparams.pkl\", \"wb\") as f:\n",
    "        pickle.dump(overall_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5219637",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best_params = pickle.load(open('./models/hyperparameters/best_hyperparams.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers = overall_best_params['DR']['PT']['num_layers']\n",
    "best_dropout = overall_best_params['DR']['PT']['dropout']\n",
    "best_model_name = overall_best_params['DR']['PT']['model_name']\n",
    "best_hidden_dim = overall_best_params['DR']['PT']['hidden_dim']\n",
    "best_num_layers, best_dropout, best_hidden_dim, best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_layers_OOL = overall_best_params['DR']['exp1']['num_layers']\n",
    "best_dropout_OOL = overall_best_params['DR']['exp1']['dropout']\n",
    "best_hidden_dim_OOL = overall_best_params['DR']['exp1']['hidden_dim']\n",
    "best_num_layers_OOL, best_dropout_OOL, best_hidden_dim_OOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75933d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a74eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment 1 = CB model EHR features\n",
    "#experiment 2 = CB model metab features\n",
    "#experiment 3 = CB model all features\n",
    "#experiment 4 = only pretrained model\n",
    "#experiment 5 = fine tune pretrained model \n",
    "#experiment 6 = add metabs, fine tune pretrained model\n",
    "\n",
    "num_iterations = 25\n",
    "\n",
    "results = {}\n",
    "for i in tqdm(outcome_list):\n",
    "    results[i] = {'exp1':[],'exp2':[],'exp3':[],'exp4':[],'exp5':[],'exp6':[]}\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        print('experiment 1')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics_data,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'EHR_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp1']['lr'], overall_best_params[i]['exp1']['lr_decay'],\n",
    "                overall_best_params[i]['exp1']['batch_size'], feature_types='EHR', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp1']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp1']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp1']['dropout'], return_preds=True)\n",
    "        results[i]['exp1'].append(val_auc)\n",
    "        \n",
    "        print('experiment 2')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics_data,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'proteomics_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp2']['lr'], overall_best_params[i]['exp2']['lr_decay'],\n",
    "                overall_best_params[i]['exp2']['batch_size'], feature_types='metab', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers, dropout=best_dropout,\n",
    "                                 return_preds=True)\n",
    "        results[i]['exp2'].append(val_auc)\n",
    "        \n",
    "        print('experiment 3')\n",
    "        val_auc = run_experiment(RNN_data_codes_omics, proteomics_data,\n",
    "                patient_indices_omics, RNN_data_outcomes_omics, RNN_data_lengths_omics, 'both_omics_only_{}'.format(j), \n",
    "                overall_best_params[i]['exp3']['lr'], overall_best_params[i]['exp3']['lr_decay'],\n",
    "                overall_best_params[i]['exp3']['batch_size'], feature_types='both', model_path='', fine_tune=False, seed=j,\n",
    "                                hidden_dim=overall_best_params[i]['exp3']['hidden_dim'],\n",
    "                                 num_layers=overall_best_params[i]['exp3']['num_layers'],\n",
    "                                 dropout=overall_best_params[i]['exp3']['dropout'],\n",
    "                                 return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp3'].append(val_auc)\n",
    "        \n",
    "        print('experiment 4')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics_data,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_PT_{}'.format(j), \n",
    "                overall_best_params[i]['exp4']['lr'], overall_best_params[i]['exp4']['lr_decay'],\n",
    "                overall_best_params[i]['exp4']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=False, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp4'].append(val_auc)\n",
    "\n",
    "        print('experiment 5')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics_data,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'EHR_PT_FT_{}'.format(j), \n",
    "                overall_best_params[i]['exp5']['lr'], overall_best_params[i]['exp5']['lr_decay'],\n",
    "                overall_best_params[i]['exp5']['batch_size'], feature_types='EHR',\n",
    "                                 model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout, return_preds=True)\n",
    "        results[i]['exp5'].append(val_auc)\n",
    "\n",
    "        print('experiment 6')\n",
    "        val_auc = run_experiment(RNN_data_codes, proteomics_data,\n",
    "                patient_indices, RNN_data_outcomes, RNN_data_lengths, 'both_PT_FT_{}'.format(j), \n",
    "                overall_best_params[i]['exp6']['lr'], overall_best_params[i]['exp6']['lr_decay'],\n",
    "                overall_best_params[i]['exp6']['batch_size'], feature_types='both',\n",
    "                                model_path='./models/predictive_models/{}.pth'.format(best_model_name),\n",
    "                                 fine_tune=True, seed=j,\n",
    "                                hidden_dim=best_hidden_dim,num_layers=best_num_layers,\n",
    "                                 dropout=best_dropout,\n",
    "                               return_preds=True, return_interpretability=True, return_grads=True)\n",
    "        results[i]['exp6'].append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Function to convert tensors to numpy arrays\n",
    "def tensor_to_numpy(x):\n",
    "    if hasattr(x, 'numpy'):  # Check if it's a tensor with numpy method\n",
    "        return x.numpy().astype(float)\n",
    "    elif 'tensor' in str(type(x)).lower():  # Check if it's a tensor by name\n",
    "        try:\n",
    "            return float(str(x).split('(')[1].split(')')[0])\n",
    "        except:\n",
    "            return float(x)\n",
    "    elif isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "    else:\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            raise ValueError(f\"Cannot convert {type(x)} to float: {x}\")\n",
    "\n",
    "# Bootstrap confidence intervals for AUROC\n",
    "def bootstrap_auroc_ci(y_true, y_pred, n_bootstraps=1000, ci=0.95):\n",
    "    # Convert to numpy arrays from any potential tensor objects\n",
    "    y_true_np = np.array([tensor_to_numpy(y) for y in y_true])\n",
    "    y_pred_np = np.array([tensor_to_numpy(y) for y in y_pred])\n",
    "    \n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        # Bootstrap by sampling with replacement\n",
    "        indices = rng.randint(0, len(y_true_np), len(y_true_np))\n",
    "        \n",
    "        # Check if the bootstrapped sample has both classes\n",
    "        if len(np.unique(y_true_np[indices])) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            score = roc_auc_score(y_true_np[indices], y_pred_np[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if len(bootstrapped_scores) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Calculate point estimate\n",
    "    auroc = roc_auc_score(y_true_np, y_pred_np)\n",
    "    \n",
    "    # Sort scores and get confidence interval bounds\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "    \n",
    "    # Compute confidence interval\n",
    "    alpha = (1.0 - ci) / 2.0\n",
    "    lower_bound = sorted_scores[int(alpha * len(sorted_scores))]\n",
    "    upper_bound = sorted_scores[int((1 - alpha) * len(sorted_scores))]\n",
    "    \n",
    "    return auroc, lower_bound, upper_bound\n",
    "\n",
    "# Bootstrap confidence intervals for AUPRC\n",
    "def bootstrap_auprc_ci(y_true, y_pred, n_bootstraps=1000, ci=0.95):\n",
    "    # Convert to numpy arrays from any potential tensor objects\n",
    "    y_true_np = np.array([tensor_to_numpy(y) for y in y_true])\n",
    "    y_pred_np = np.array([tensor_to_numpy(y) for y in y_pred])\n",
    "    \n",
    "    bootstrapped_scores = []\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        # Bootstrap by sampling with replacement\n",
    "        indices = rng.randint(0, len(y_true_np), len(y_true_np))\n",
    "        \n",
    "        # Check if the bootstrapped sample has both classes\n",
    "        if len(np.unique(y_true_np[indices])) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            score = average_precision_score(y_true_np[indices], y_pred_np[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if len(bootstrapped_scores) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Calculate point estimate\n",
    "    auprc = average_precision_score(y_true_np, y_pred_np)\n",
    "    \n",
    "    # Sort scores and get confidence interval bounds\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "    \n",
    "    # Compute confidence interval\n",
    "    alpha = (1.0 - ci) / 2.0\n",
    "    lower_bound = sorted_scores[int(alpha * len(sorted_scores))]\n",
    "    upper_bound = sorted_scores[int((1 - alpha) * len(sorted_scores))]\n",
    "    \n",
    "    return auprc, lower_bound, upper_bound\n",
    "\n",
    "# Main code with minimal output for both metrics\n",
    "for exp in results['DR'].keys():\n",
    "    true_outcomes = []\n",
    "    total_preds = []\n",
    "    indices = []\n",
    "    \n",
    "    for i in results['DR'][exp]:\n",
    "        true_outcomes.extend(i[3])\n",
    "        total_preds.extend(i[4])\n",
    "        indices.extend(i[5])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([true_outcomes, total_preds, indices]).T\n",
    "    df.columns = ['true_outcome', 'pred', 'index']\n",
    "    \n",
    "    try:\n",
    "        # Convert tensors to numeric values\n",
    "        df['true_outcome_num'] = df['true_outcome'].apply(tensor_to_numpy)\n",
    "        df['pred_num'] = df['pred'].apply(tensor_to_numpy)\n",
    "        df['index_num'] = df['index'].apply(tensor_to_numpy)\n",
    "        \n",
    "        # Group by the numeric index\n",
    "        df_grouped = df.groupby('index_num')[['true_outcome_num', 'pred_num']].mean()\n",
    "        \n",
    "        # Calculate AUROC and bootstrap CI\n",
    "        auroc, auroc_lower, auroc_upper = bootstrap_auroc_ci(\n",
    "            df_grouped['true_outcome_num'], \n",
    "            df_grouped['pred_num'], \n",
    "            n_bootstraps=2000,  # Using more bootstraps for stability\n",
    "            ci=0.95\n",
    "        )\n",
    "        \n",
    "        # Calculate AUPRC and bootstrap CI\n",
    "        auprc, auprc_lower, auprc_upper = bootstrap_auprc_ci(\n",
    "            df_grouped['true_outcome_num'], \n",
    "            df_grouped['pred_num'], \n",
    "            n_bootstraps=10000,  # Using more bootstraps for stability\n",
    "            ci=0.95\n",
    "        )\n",
    "        \n",
    "        # Print results for both metrics\n",
    "        print(f\"{exp}:\")\n",
    "        print(f\"  AUROC = {auroc:.4f}, 95% CI = [{auroc_lower:.4f}, {auroc_upper:.4f}]\")\n",
    "        print(f\"  AUPRC = {auprc:.4f}, 95% CI = [{auprc_lower:.4f}, {auprc_upper:.4f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{exp}: Error calculating metrics - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf31124",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./results/results.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(results,f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edeb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('./results/results.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_val = ttest_rel([i[0] for i in results['DR']['exp3']], [i[0] for i in results['DR']['exp6']])\n",
    "\n",
    "print(f'The p-value is {p_val}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4de73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
